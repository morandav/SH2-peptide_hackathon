{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-k0q29Z3ukKI",
        "outputId": "b30061f9-219b-469e-83b0-2ccf279399b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Bio\n",
            "  Downloading bio-1.3.9-py3-none-any.whl (270 kB)\n",
            "\u001b[K     |████████████████████████████████| 270 kB 12.9 MB/s \n",
            "\u001b[?25hCollecting biopython>=1.79\n",
            "  Downloading biopython-1.79-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 54.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from Bio) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from Bio) (2.23.0)\n",
            "Collecting mygene\n",
            "  Downloading mygene-3.2.2-py2.py3-none-any.whl (5.4 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from biopython>=1.79->Bio) (1.21.6)\n",
            "Collecting biothings-client>=0.2.6\n",
            "  Downloading biothings_client-0.2.6-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->Bio) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->Bio) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->Bio) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->Bio) (2.10)\n",
            "Installing collected packages: biothings-client, mygene, biopython, Bio\n",
            "Successfully installed Bio-1.3.9 biopython-1.79 biothings-client-0.2.6 mygene-3.2.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting import-ipynb\n",
            "  Downloading import_ipynb-0.1.4-py3-none-any.whl (4.1 kB)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from import-ipynb) (5.4.0)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (from import-ipynb) (5.5.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (0.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (57.4.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (5.1.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (2.6.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (1.0.18)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (0.7.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->import-ipynb) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->import-ipynb) (1.15.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat->import-ipynb) (4.10.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->import-ipynb) (4.3.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->import-ipynb) (2.15.3)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (5.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (21.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (4.2.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (4.11.4)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (0.18.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat->import-ipynb) (3.8.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->IPython->import-ipynb) (0.7.0)\n",
            "Installing collected packages: import-ipynb\n",
            "Successfully installed import-ipynb-0.1.4\n"
          ]
        }
      ],
      "source": [
        "# delete this cell if working on Pycharm\n",
        "!pip install Bio\n",
        "!pip install import-ipynb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from Bio.PDB import *\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "ZfvL-xd2u8uj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUNB3ovQKYlS",
        "outputId": "e07a9df5-b109-4ef8-a596-35b9a5eaaa37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n",
            "importing Jupyter notebook from utils.ipynb\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Bio in /usr/local/lib/python3.7/dist-packages (1.3.9)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from Bio) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from Bio) (2.23.0)\n",
            "Requirement already satisfied: biopython>=1.79 in /usr/local/lib/python3.7/dist-packages (from Bio) (1.79)\n",
            "Requirement already satisfied: mygene in /usr/local/lib/python3.7/dist-packages (from Bio) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from biopython>=1.79->Bio) (1.21.6)\n",
            "Requirement already satisfied: biothings-client>=0.2.6 in /usr/local/lib/python3.7/dist-packages (from mygene->Bio) (0.2.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->Bio) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->Bio) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->Bio) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->Bio) (2.10)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# so we can import utils notebook (delete if working on Pycharm), you might need to change it to your working directory path\n",
        "%cd \"/content/drive/MyDrive/Colab Notebooks/\" \n",
        "\n",
        "import import_ipynb\n",
        "import utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrW415mXmb_h",
        "outputId": "23f9d17b-258c-4b53-e889-f848f92fc928"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "lHPbw7s8KYlV"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = utils.SH2_MAX_LEN + utils.PEP_MAX_LEN\n",
        "FEATURE_NUM = utils.FEATURE_NUM\n",
        "\n",
        "# output column size\n",
        "DENSE_UNITS = 15\n",
        "MODEL_PATH = \"SH2_and_pep_model\"\n",
        "\n",
        "###############################################################################\n",
        "#                                                                             \n",
        "#              Parameters you can change, but don't have to                   #\n",
        "#                                                                             #\n",
        "###############################################################################\n",
        "\n",
        "\n",
        "# number of ResNet blocks for the first ResNet and the kernel size.\n",
        "RESNET_1_BLOCKS = 3\n",
        "RESNET_1_KERNEL_SIZE = 15\n",
        "RESNET_1_KERNEL_NUM = 32\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "#                                                                             #\n",
        "#                        Parameters you need to choose                        #\n",
        "#                                                                             #\n",
        "###############################################################################\n",
        "\n",
        "\n",
        "# number of ResNet blocks for the second ResNet, dilation list to repeat and the kernel size.\n",
        "\n",
        "RESNET_2_BLOCKS = 1\n",
        "RESNET_2_KERNEL_SIZE = 5  # good start may be 3/5\n",
        "RESNET_2_KERNEL_NUM = 64\n",
        "DILATION = [1,2,4, 8, 16]\n",
        "\n",
        "# percentage of dropout for the dropout layer\n",
        "DROPOUT = 0.1 # good start may be 0.1-0.5\n",
        "\n",
        "# number of epochs, Learning rate and Batch size\n",
        "EPOCHS = 150\n",
        "LR = 0.003 # good start may be 0.0001/0.001/0.01\n",
        "BATCH = 30 # good start may be 32/64/128\n",
        "\n",
        "TEST_PERCENT = 10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3EJS15XdKYla"
      },
      "outputs": [],
      "source": [
        "def resnet_1(input_layer):  # TODO: implement this!\n",
        "    \"\"\"\n",
        "    ResNet layer - input -> BatchNormalization -> Conv1D -> Relu -> BatchNormalization -> Conv1D -> Relu -> Add\n",
        "    :param input_layer: input layer for the ResNet\n",
        "    :return: last layer of the ResNet\n",
        "    \"\"\"\n",
        "    for k in range(RESNET_1_BLOCKS):\n",
        "      batch_layer = tf.keras.layers.BatchNormalization()(input_layer)\n",
        "      conv1d_layer = layers.Conv1D(RESNET_1_KERNEL_NUM, RESNET_1_KERNEL_SIZE, padding='same')(batch_layer)\n",
        "      relu_layer = tf.keras.layers.ReLU()(conv1d_layer)\n",
        "      batch_layer = tf.keras.layers.BatchNormalization()(relu_layer)\n",
        "      conv1d_layer = layers.Conv1D(RESNET_1_KERNEL_NUM, RESNET_1_KERNEL_SIZE, padding='same')(batch_layer)\n",
        "      relu_layer = tf.keras.layers.ReLU()(conv1d_layer)\n",
        "      input_layer = layers.Add()([input_layer, relu_layer])\n",
        "    return input_layer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Quv773Q8KYld"
      },
      "outputs": [],
      "source": [
        "def resnet_2(input_layer):  # TODO: implement this!\n",
        "    \"\"\"\n",
        "    Dilated ResNet layer - input -> BatchNormalization -> dilated Conv1D -> Relu -> BatchNormalization -> dilated Conv1D -> Relu -> Add\n",
        "    :param input_layer: input layer for the ResNet\n",
        "    :return: last layer of the ResNet\n",
        "    \"\"\"\n",
        "    for n in range(RESNET_2_BLOCKS):\n",
        "       for m in DILATION:\n",
        "          batch_layer = tf.keras.layers.BatchNormalization()(input_layer)\n",
        "          conv1d_layer = layers.Conv1D(RESNET_2_KERNEL_NUM, RESNET_2_KERNEL_SIZE, dilation_rate=m, padding='same')(batch_layer)\n",
        "          relu_layer = tf.keras.layers.ReLU()(conv1d_layer)\n",
        "          batch_layer = tf.keras.layers.BatchNormalization()(relu_layer)\n",
        "          conv1d_layer = layers.Conv1D(RESNET_2_KERNEL_NUM, RESNET_2_KERNEL_SIZE, dilation_rate=m, padding='same')(batch_layer)\n",
        "          relu_layer = tf.keras.layers.ReLU()(conv1d_layer)\n",
        "          input_layer = layers.Add()([input_layer, relu_layer])\n",
        "    return input_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pX74QxOaKYlg"
      },
      "outputs": [],
      "source": [
        "def build_network():\n",
        "    \"\"\"\n",
        "    builds the neural network architecture as shown in the exercise.\n",
        "    :return: a Keras Model\n",
        "    \"\"\"\n",
        "    # input, shape (MAX_LENGTH,FEATURE_NUM)\n",
        "    input_layer = tf.keras.Input(shape=(MAX_LEN, FEATURE_NUM))\n",
        "\n",
        "    # Conv1D -> shape = (MAX_LENGTH, RESNET_1_KERNEL_NUM)\n",
        "    conv1d_layer = layers.Conv1D(RESNET_1_KERNEL_NUM, RESNET_1_KERNEL_SIZE, padding='same')(input_layer)\n",
        "\n",
        "    # first ResNet -> shape = (MAX_LENGTH, RESNET_1_KERNEL_NUM)\n",
        "    resnet_layer = resnet_1(conv1d_layer)\n",
        "\n",
        "    # Conv1D -> shape = (MAX_LENGTH, RESNET_2_KERNEL_NUM)\n",
        "    conv1d_layer = layers.Conv1D(RESNET_2_KERNEL_NUM, RESNET_2_KERNEL_SIZE, padding=\"same\")(resnet_layer)\n",
        "\n",
        "    # second ResNet -> shape = (MAX_LENGTH, RESNET_2_KERNEL_NUM)\n",
        "    resnet_layer = resnet_2(conv1d_layer)\n",
        "\n",
        "    # Dropout layer\n",
        "    dropout_layer = tf.keras.layers.Dropout(DROPOUT)(resnet_layer)\n",
        "\n",
        "    # Conv1D layer\n",
        "    conv1d_layer = layers.Conv1D(RESNET_2_KERNEL_NUM//2, RESNET_2_KERNEL_SIZE, padding=\"same\")(dropout_layer)\n",
        "\n",
        "    # Elu activation layer\n",
        "    elu_layer = tf.keras.layers.ELU()(conv1d_layer)\n",
        "\n",
        "    # Dense layer\n",
        "    dense_layer = tf.keras.layers.Dense(DENSE_UNITS)(elu_layer)\n",
        "\n",
        "    # create model\n",
        "    model = tf.keras.Model(input_layer, dense_layer)\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EPMxJcPiKYlj"
      },
      "outputs": [],
      "source": [
        "def plot_val_train_loss(history):\n",
        "    \"\"\"\n",
        "    plots the train and validation loss of the model at each epoch, saves it in 'model_loss_history.png'\n",
        "    :param history: history object (output of fit function)\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    ig, axes = plt.subplots(1, 1, figsize=(15,3))\n",
        "    loss_hist = history.history['loss']\n",
        "    # for i  in range(len(loss_hist)):\n",
        "      # loss_hist[i] = loss_hist[i]/100\n",
        "\n",
        "    axes.plot(loss_hist, label='Training loss')\n",
        "\n",
        "    val_loss = history.history['val_loss']\n",
        "    # for i in range(len(val_loss)):\n",
        "      # val_loss[i] = val_loss[i]/100\n",
        "\n",
        "    axes.plot(val_loss, label='Validation loss')\n",
        "    axes.legend()\n",
        "    axes.set_title(\"Train and Val MSE loss\")\n",
        "\n",
        "    plt.savefig(\"./model_loss_history\")  # TODO: you can change the path here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EYxGEnJbKYlm",
        "outputId": "19f53593-9345-471e-940c-623a69f02402"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "3/3 [==============================] - 6s 680ms/step - loss: 50.4910 - val_loss: 46.9477\n",
            "Epoch 2/150\n",
            "3/3 [==============================] - 1s 367ms/step - loss: 32.3181 - val_loss: 17.9374\n",
            "Epoch 3/150\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 9.1729 - val_loss: 16.9517\n",
            "Epoch 4/150\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 6.7579 - val_loss: 23.2929\n",
            "Epoch 5/150\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 5.3828 - val_loss: 24.3765\n",
            "Epoch 6/150\n",
            "3/3 [==============================] - 1s 368ms/step - loss: 4.5913 - val_loss: 17.7783\n",
            "Epoch 7/150\n",
            "3/3 [==============================] - 1s 366ms/step - loss: 4.2144 - val_loss: 18.7051\n",
            "Epoch 8/150\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 3.6970 - val_loss: 18.6469\n",
            "Epoch 9/150\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 3.2882 - val_loss: 16.9287\n",
            "Epoch 10/150\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 2.9476 - val_loss: 16.5512\n",
            "Epoch 11/150\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 2.6093 - val_loss: 15.5729\n",
            "Epoch 12/150\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 2.4957 - val_loss: 12.7161\n",
            "Epoch 13/150\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 2.2927 - val_loss: 11.2348\n",
            "Epoch 14/150\n",
            "3/3 [==============================] - 1s 369ms/step - loss: 2.1904 - val_loss: 11.4465\n",
            "Epoch 15/150\n",
            "3/3 [==============================] - 1s 369ms/step - loss: 2.2187 - val_loss: 9.8039\n",
            "Epoch 16/150\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 2.0216 - val_loss: 9.0974\n",
            "Epoch 17/150\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 1.9480 - val_loss: 8.7988\n",
            "Epoch 18/150\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 1.8821 - val_loss: 7.4246\n",
            "Epoch 19/150\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 1.7701 - val_loss: 7.4885\n",
            "Epoch 20/150\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 1.7262 - val_loss: 6.1667\n",
            "Epoch 21/150\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 1.7230 - val_loss: 5.7415\n",
            "Epoch 22/150\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 1.6619 - val_loss: 5.6983\n",
            "Epoch 23/150\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 1.6176 - val_loss: 5.4280\n",
            "Epoch 24/150\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 1.5860 - val_loss: 4.5833\n",
            "Epoch 25/150\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 1.6484 - val_loss: 4.0576\n",
            "Epoch 26/150\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 1.6739 - val_loss: 4.5388\n",
            "Epoch 27/150\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 1.5966 - val_loss: 4.6008\n",
            "Epoch 28/150\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 1.7343 - val_loss: 3.8207\n",
            "Epoch 29/150\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 1.4977 - val_loss: 3.3826\n",
            "Epoch 30/150\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 1.4830 - val_loss: 3.3912\n",
            "Epoch 31/150\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 1.4573 - val_loss: 3.6311\n",
            "Epoch 32/150\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 1.4275 - val_loss: 3.1871\n",
            "Epoch 33/150\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 1.3884 - val_loss: 3.0745\n",
            "Epoch 34/150\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 1.4614 - val_loss: 2.7461\n",
            "Epoch 35/150\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 1.4250 - val_loss: 2.5567\n",
            "Epoch 36/150\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 1.4626 - val_loss: 2.8275\n",
            "Epoch 37/150\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 1.3758 - val_loss: 2.8911\n",
            "Epoch 38/150\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 1.3718 - val_loss: 2.4660\n",
            "Epoch 39/150\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 1.3583 - val_loss: 2.4111\n",
            "Epoch 40/150\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 1.3958 - val_loss: 2.1861\n",
            "Epoch 41/150\n",
            "3/3 [==============================] - 1s 491ms/step - loss: 1.3137 - val_loss: 2.7903\n",
            "Epoch 42/150\n",
            "3/3 [==============================] - 1s 484ms/step - loss: 1.3855 - val_loss: 2.4485\n",
            "Epoch 43/150\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 1.3942 - val_loss: 2.0761\n",
            "Epoch 44/150\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 1.3187 - val_loss: 2.2713\n",
            "Epoch 45/150\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 1.3249 - val_loss: 2.1941\n",
            "Epoch 46/150\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 1.2542 - val_loss: 2.2724\n",
            "Epoch 47/150\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 1.2362 - val_loss: 2.1416\n",
            "Epoch 48/150\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 1.2816 - val_loss: 2.0971\n",
            "Epoch 49/150\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 1.2839 - val_loss: 2.1244\n",
            "Epoch 50/150\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 1.2310 - val_loss: 2.2109\n",
            "Epoch 51/150\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 1.2397 - val_loss: 2.0935\n",
            "Epoch 52/150\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 1.2277 - val_loss: 2.0604\n",
            "Epoch 53/150\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 1.2157 - val_loss: 2.1422\n",
            "Epoch 54/150\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 1.1745 - val_loss: 2.1328\n",
            "Epoch 55/150\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 1.2096 - val_loss: 2.2025\n",
            "Epoch 56/150\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 1.2245 - val_loss: 2.1963\n",
            "Epoch 57/150\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 1.3121 - val_loss: 2.2443\n",
            "Epoch 58/150\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 1.3208 - val_loss: 2.2054\n",
            "Epoch 59/150\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 1.2302 - val_loss: 2.2164\n",
            "Epoch 60/150\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 1.2473 - val_loss: 2.0988\n",
            "Epoch 61/150\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 1.1237 - val_loss: 2.2345\n",
            "Epoch 62/150\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 1.1824 - val_loss: 2.0129\n",
            "Epoch 63/150\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 1.1601 - val_loss: 2.0563\n",
            "Epoch 64/150\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 1.1379 - val_loss: 2.0582\n",
            "Epoch 65/150\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 1.1287 - val_loss: 2.0992\n",
            "Epoch 66/150\n",
            "3/3 [==============================] - 1s 369ms/step - loss: 1.1475 - val_loss: 2.1947\n",
            "Epoch 67/150\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 1.1431 - val_loss: 2.1031\n",
            "Epoch 68/150\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 1.1392 - val_loss: 2.1025\n",
            "Epoch 69/150\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 1.1897 - val_loss: 2.2693\n",
            "Epoch 70/150\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 1.3111 - val_loss: 1.9839\n",
            "Epoch 71/150\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 1.1843 - val_loss: 2.3259\n",
            "Epoch 72/150\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 1.1939 - val_loss: 1.9777\n",
            "Epoch 73/150\n",
            "3/3 [==============================] - 1s 369ms/step - loss: 1.1410 - val_loss: 2.0696\n",
            "Epoch 74/150\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 1.1350 - val_loss: 2.0119\n",
            "Epoch 75/150\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 1.0688 - val_loss: 2.0200\n",
            "Epoch 76/150\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 1.0795 - val_loss: 2.0803\n",
            "Epoch 77/150\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 1.0573 - val_loss: 2.0231\n",
            "Epoch 78/150\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 1.0777 - val_loss: 2.0499\n",
            "Epoch 79/150\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 1.0922 - val_loss: 1.9472\n",
            "Epoch 80/150\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 1.1364 - val_loss: 2.0576\n",
            "Epoch 81/150\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 1.0907 - val_loss: 2.3348\n",
            "Epoch 82/150\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 1.1211 - val_loss: 2.0771\n",
            "Epoch 83/150\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 1.0659 - val_loss: 1.9743\n",
            "Epoch 84/150\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 1.0492 - val_loss: 1.9795\n",
            "Epoch 85/150\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 1.0566 - val_loss: 2.0197\n",
            "Epoch 86/150\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 1.0451 - val_loss: 2.0510\n",
            "Epoch 87/150\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 1.0634 - val_loss: 2.0736\n",
            "Epoch 88/150\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 1.0054 - val_loss: 2.0097\n",
            "Epoch 89/150\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 1.0130 - val_loss: 2.0686\n",
            "Epoch 90/150\n",
            "3/3 [==============================] - 1s 368ms/step - loss: 1.0843 - val_loss: 1.9062\n",
            "Epoch 91/150\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 1.1116 - val_loss: 2.0271\n",
            "Epoch 92/150\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 1.1170 - val_loss: 2.0540\n",
            "Epoch 93/150\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 1.0130 - val_loss: 1.9608\n",
            "Epoch 94/150\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 1.0171 - val_loss: 2.0663\n",
            "Epoch 95/150\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 1.0094 - val_loss: 1.9131\n",
            "Epoch 96/150\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 1.0329 - val_loss: 1.9447\n",
            "Epoch 97/150\n",
            "3/3 [==============================] - 1s 368ms/step - loss: 1.0940 - val_loss: 1.9199\n",
            "Epoch 98/150\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 1.0621 - val_loss: 2.0205\n",
            "Epoch 99/150\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 1.1414 - val_loss: 2.2107\n",
            "Epoch 100/150\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 1.3675 - val_loss: 2.7444\n",
            "Epoch 101/150\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 1.5132 - val_loss: 2.2247\n",
            "Epoch 102/150\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 1.4217 - val_loss: 2.0616\n",
            "Epoch 103/150\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 1.3854 - val_loss: 2.4796\n",
            "Epoch 104/150\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 1.4021 - val_loss: 2.0957\n",
            "Epoch 105/150\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 1.2803 - val_loss: 2.6086\n",
            "Epoch 106/150\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 1.2261 - val_loss: 2.0301\n",
            "Epoch 107/150\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 1.1687 - val_loss: 2.0697\n",
            "Epoch 108/150\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 1.2400 - val_loss: 2.0714\n",
            "Epoch 109/150\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 1.1554 - val_loss: 2.3513\n",
            "Epoch 110/150\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 1.1662 - val_loss: 2.3036\n",
            "Epoch 111/150\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 1.0576 - val_loss: 2.0141\n",
            "Epoch 112/150\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 1.0511 - val_loss: 2.2351\n",
            "Epoch 113/150\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 1.0291 - val_loss: 2.2269\n",
            "Epoch 114/150\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 1.0046 - val_loss: 2.0052\n",
            "Epoch 115/150\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 0.9876 - val_loss: 1.9330\n",
            "Epoch 116/150\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.9941 - val_loss: 2.2129\n",
            "Epoch 117/150\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 1.0975 - val_loss: 2.1258\n",
            "Epoch 118/150\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 1.0524 - val_loss: 1.9618\n",
            "Epoch 119/150\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 1.0260 - val_loss: 1.9517\n",
            "Epoch 120/150\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 1.0144 - val_loss: 2.0045\n",
            "Epoch 121/150\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.9986 - val_loss: 2.1829\n",
            "Epoch 122/150\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 1.0900 - val_loss: 2.0005\n",
            "Epoch 123/150\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 1.0613 - val_loss: 1.8677\n",
            "Epoch 124/150\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.9873 - val_loss: 1.9519\n",
            "Epoch 125/150\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.9548 - val_loss: 2.0078\n",
            "Epoch 126/150\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.9610 - val_loss: 1.9922\n",
            "Epoch 127/150\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.9630 - val_loss: 2.0162\n",
            "Epoch 128/150\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.9382 - val_loss: 1.9709\n",
            "Epoch 129/150\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.9620 - val_loss: 2.0150\n",
            "Epoch 130/150\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 0.9792 - val_loss: 2.0144\n",
            "Epoch 131/150\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 0.9871 - val_loss: 1.9579\n",
            "Epoch 132/150\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.9971 - val_loss: 2.1061\n",
            "Epoch 133/150\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.9521 - val_loss: 1.9707\n",
            "Epoch 134/150\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.9336 - val_loss: 1.9221\n",
            "Epoch 135/150\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.9589 - val_loss: 2.0542\n",
            "Epoch 136/150\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 0.9257 - val_loss: 1.9897\n",
            "Epoch 137/150\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 0.9253 - val_loss: 1.9625\n",
            "Epoch 138/150\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.9197 - val_loss: 1.9054\n",
            "Epoch 139/150\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 0.9471 - val_loss: 1.9330\n",
            "Epoch 140/150\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.9293 - val_loss: 1.9212\n",
            "Epoch 141/150\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.9051 - val_loss: 2.0516\n",
            "Epoch 142/150\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 0.9331 - val_loss: 1.9345\n",
            "Epoch 143/150\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.8997 - val_loss: 1.9549\n",
            "Epoch 144/150\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 0.9205 - val_loss: 1.9076\n",
            "Epoch 145/150\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.9198 - val_loss: 1.9247\n",
            "Epoch 146/150\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.9028 - val_loss: 1.9530\n",
            "Epoch 147/150\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.8903 - val_loss: 1.9932\n",
            "Epoch 148/150\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.9392 - val_loss: 2.1217\n",
            "Epoch 149/150\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.9620 - val_loss: 1.9301\n",
            "Epoch 150/150\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.8929 - val_loss: 1.9540\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x216 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2cAAADSCAYAAADOpLg+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeZxcVZ338c+v9t67s6+QhCUBkpBAAkgkLG4gKIiIMDxCREEdxw1HRZ0RHIdnnJF5ZHhEH1GEqMwA6ogo4AIEAjIiSUDWRAgkkAXSSXrvru5azvPHudVV3elOdyfdVZ309/161etW3aq691d3q/s759xzzTmHiIiIiIiIlFao1AGIiIiIiIiIkjMREREREZFRQcmZiIiIiIjIKKDkTEREREREZBRQciYiIiIiIjIKKDkTEREREREZBZSciYjIoJnZ/WZ22SiI41oz++kITHeFmT023NPdH2bmzOzwUschIiIjT8mZiMhBzsxaCx5ZM+soeH3JUKblnDvLObdypGLdX2Y23czSZnZYH+/90syu349pzwoSpad6jZ9gZl1mtqlg3FvN7HEzazKz3Wb2RzNbGry3wswyvdZLq5lN29fYRETk4KDkTETkIOecq8w9gNeA9xSMuz33OTOLlC7K4eGc2wo8CHyocLyZjQPeDQxHYlluZvMLXv8N8GrBvKqB3wD/FxgHTAe+DnQWfOd/CtdL8Ng2DLGJiMgBTMmZiMgYZWanmdkWM/uSmb0B3GpmdWb2GzOrN7OG4PmMgu88bGYfDZ6vMLPHzOz64LOvmtlZe5nf1Wa20cxazOwFM3tfwXt7nZaZzTazR4Lv/gGYsJeftpJeyRlwEfCCc+7ZvcUxSD8BCpt2Xgr8uOD1kQDOuf9yzmWccx3Oud87554Z4nz2YGY1ZvbjYP1sNrN/MLNQ8N7hwTJqMrOdZnZnMN7M7NtmtsPMms3s2V7JpYiIjBJKzkRExrYp+NqdQ4Er8f8LtwavDwE6gO/s5fsnAhvwydK/AbeYmfXz2Y3AKUANvibpp2Y2dZDT+k9gbfDeN+iZHPX2S2CCmb21YNyHyNeaDRTHQH4KXGRmYTM7GqgEnih4/69AxsxWmtlZZlY3hGkP5P/i454DnIpPDD8cvPcN4PdAHTAj+CzAO4Hl+KSxBrgQ2DWMMYmIyDBRciYiMrZlgWucc51BDc8u59wvnHPtzrkW4Dp8EtCfzc65HzjnMvjkZyowua8POud+5pzb5pzLOufuBF4CThhoWmZ2CLAU+McgztXAr/sLyDnXAfwMn7hgZkcAx+MTvMHEMZAt+CTy7cE8ftJr/s3AWwEH/ACoN7N7zKxwuZxkZo0Fj40DzdTMwvgawC8751qcc5uAfydfS5jCJ9XTnHNJ59xjBeOrgHmAOededM5tH8LvFRGRIlFyJiIyttU755K5F2ZWbmbfD5rMNQOrgdogMejLG7knzrn24GllXx80s0vN7OlcQgLMp2fzxP6mNQ1ocM61FXx28wC/ayXwATNL4JOX3znndgwyjsH4MbACuJheyVkQ/4vOuRXOuRnB9KcBNxR85E/OudqCxx4dmPRhAhCl52/fjL+mDeCLgAF/NrPnzezyIJaH8LWfNwE7zOzm4Lo4EREZZZSciYiMba7X688Dc4ETnXPV+OZw4E/695mZHYqvRfo7YLxzrhZ4bpDT3Q7UmVlFwbhDBvjOY8Bu4FzgfxE0adzPOAr9AjgbeMU599rePuicWw/chk/S9sdO8rVjOYcAW4P5vOGcu8I5Nw34GPBdC7rgd87d6Jw7Hjga37zxC/sZi4iIjAAlZyIiUqgKf51ZY9DD4TXDNN0KfCJYD2BmH2aQyYpzbjOwBvi6mcWCa8neM8B3HL5261+BWvLNIPc5jl7TbwPOAD7a+z0zm2dmn891pGJmM/E1bH8a6nx6zTMD3AVcZ2ZVQaJ5Ff4aOMzsAwWdtzTgf2fWzJaa2YlmFgXagCS+OauIiIwySs5ERKTQDUAZvpbmT8Bvh2OizrkX8NdH/Q/wJrAA+OMQJvE3+A5DduMTxh/v/eMQfOYQ4E7nXOcwxdHNObfGOdfXtWItQaxPmFkbfjk+h6+VzHmL7Xmfs6WDmO2n8AnWK/jawf8EfhS8tzSYZytwD/AZ59wrQDW+trAB3wxyF/CtIf5cEREpAvOFiyIiIiIiIlJKqjkTEREREREZBZSciYiIiIiIjAJKzkREREREREYBJWciIiIiIiKjgJIzERERERGRUSBSzJlNmDDBzZo1q5izFBERERERGTXWrl270zk3sa/3ipqczZo1izVr1hRzliIiIiIiIqOGmW3u7z01axQRERERERkFlJyJiIiIiIiMAkrORERERERERoFBXXNmZpuAFiADpJ1zS8xsHHAnMAvYBFzonGsYmTBFRERERCSVSrFlyxaSyWSpQ5EBJBIJZsyYQTQaHfR3htIhyOnOuZ0Fr68GHnTOfdPMrg5ef2kI0xsV/vd9L3L4xEouXDqz1KGIiIiIiOzVli1bqKqqYtasWZhZqcORfjjn2LVrF1u2bGH27NmD/t7+NGs8F1gZPF8JnLcf0yqZB154k9Uv1Zc6DBERERGRASWTScaPH6/EbJQzM8aPHz/kGs7BJmcO+L2ZrTWzK4Nxk51z24PnbwCThzTnUWJCVZz6ls5ShyEiIiIiMihKzA4M+7KeBpucvdU5dxxwFvBJM1te+KZzzuETuL6CutLM1pjZmvr60VdDNbEqTn2rkjMRERERkYHs2rWLRYsWsWjRIqZMmcL06dO7X3d1de31u2vWrOHTn/70gPM4+eSThyXWhx9+mHPOOWdYplUsg7rmzDm3NRjuMLNfAicAb5rZVOfcdjObCuzo57s3AzcDLFmypM8ErpQmVsZZrZozEREREZEBjR8/nqeffhqAa6+9lsrKSv7+7/+++/10Ok0k0neKsWTJEpYsWTLgPB5//PHhCfYANGDNmZlVmFlV7jnwTuA54B7gsuBjlwG/GqkgR9LEqjgtyTTJVKbUoYiIiIiIHHBWrFjBxz/+cU488US++MUv8uc//5m3vOUtLF68mJNPPpkNGzYAPWuyrr32Wi6//HJOO+005syZw4033tg9vcrKyu7Pn3baaVxwwQXMmzePSy65BN9gD+677z7mzZvH8ccfz6c//ekBa8h2797Neeedx8KFCznppJN45plnAHjkkUe6a/4WL15MS0sL27dvZ/ny5SxatIj58+fz6KOPDvsy689gas4mA78M2kxGgP90zv3WzJ4E7jKzjwCbgQtHLsyRM7EqDkB9Syczx5WXOBoRERERkcH5+q+f54VtzcM6zaOnVXPNe44Z8ve2bNnC448/Tjgcprm5mUcffZRIJMIDDzzAV77yFX7xi1/s8Z3169ezatUqWlpamDt3Lp/4xCf26Hb+qaee4vnnn2fatGksW7aMP/7xjyxZsoSPfexjrF69mtmzZ3PxxRcPGN8111zD4sWLufvuu3nooYe49NJLefrpp7n++uu56aabWLZsGa2trSQSCW6++Wbe9a538dWvfpVMJkN7e/uQl8e+GjA5c869Ahzbx/hdwNtGIqhiyiVnO5SciYiIiIjskw984AOEw2EAmpqauOyyy3jppZcwM1KpVJ/fOfvss4nH48TjcSZNmsSbb77JjBkzenzmhBNO6B63aNEiNm3aRGVlJXPmzOnuov7iiy/m5ptv3mt8jz32WHeCeMYZZ7Br1y6am5tZtmwZV111FZdccgnnn38+M2bMYOnSpVx++eWkUinOO+88Fi1atF/LZiiGcp+zg9LEynzNmYiIiIjIgWJfarhGSkVFRffzf/zHf+T000/nl7/8JZs2beK0007r8zvxeLz7eTgcJp1O79Nn9sfVV1/N2WefzX333ceyZcv43e9+x/Lly1m9ejX33nsvK1as4KqrruLSSy8d1vn2Z3/uc3ZQmJRr1qgeG0VERERE9ltTUxPTp08H4Lbbbhv26c+dO5dXXnmFTZs2AXDnnXcO+J1TTjmF22+/HfDXsk2YMIHq6mo2btzIggUL+NKXvsTSpUtZv349mzdvZvLkyVxxxRV89KMfZd26dcP+G/oz5pOzcWVhEtalmjMRERERkWHwxS9+kS9/+cssXrx42Gu6AMrKyvjud7/LmWeeyfHHH09VVRU1NTV7/c61117L2rVrWbhwIVdffTUrV64E4IYbbmD+/PksXLiQaDTKWWedxcMPP8yxxx7L4sWLufPOO/nMZz4z7L+hP5br8aQYlixZ4tasWVO0+Q3Kv8/j7tajeWLB1/mX8xeUOhoRERERkX69+OKLHHXUUaUOo+RaW1uprKzEOccnP/lJjjjiCD73uc+VOqw99LW+zGytc67PewqM+Zoz4lWMjyRVcyYiIiIicoD4wQ9+wKJFizjmmGNoamriYx/7WKlDGhZjvkMQ4tXUhjp0zZmIiIiIyAHic5/73KisKdtfqjlL1FBtHexUzZmIiIiIiJSQkrNENZW0U9/SSTGvvxMRERERESmk5CxeTVm2ja5MluaO4e9NRkREREREZDCUnCWqiWVaAahvTZY4GBERERERGauUnMVriGSSREizQ9ediYiIiIj06/TTT+d3v/tdj3E33HADn/jEJ/r9zmmnnUbudlrvfve7aWxs3OMz1157Lddff/1e53333XfzwgsvdL/+2te+xgMPPDCU8Pv08MMPc8455+z3dIaDkrNENQBVwXVnIiIiIiLSt4svvpg77rijx7g77riDiy++eFDfv++++6itrd2nefdOzv7pn/6Jt7/97fs0rdFKyVk8SM6sQ8mZiIiIiMheXHDBBdx77710dXUBsGnTJrZt28Ypp5zCJz7xCZYsWcIxxxzDNddc0+f3Z82axc6dOwG47rrrOPLII3nrW9/Khg0buj/zgx/8gKVLl3Lsscfy/ve/n/b2dh5//HHuuecevvCFL7Bo0SI2btzIihUr+PnPfw7Agw8+yOLFi1mwYAGXX345nZ2d3fO75pprOO6441iwYAHr16/f6+/bvXs35513HgsXLuSkk07imWeeAeCRRx5h0aJFLFq0iMWLF9PS0sL27dtZvnw5ixYtYv78+Tz66KP7t3DRfc66a87GhXUjahERERE5gNx/Nbzx7PBOc8oCOOub/b49btw4TjjhBO6//37OPfdc7rjjDi688ELMjOuuu45x48aRyWR429vexjPPPMPChQv7nM7atWu54447ePrpp0mn0xx33HEcf/zxAJx//vlcccUVAPzDP/wDt9xyC5/61Kd473vfyznnnMMFF1zQY1rJZJIVK1bw4IMPcuSRR3LppZfyve99j89+9rMATJgwgXXr1vHd736X66+/nh/+8If9/r5rrrmGxYsXc/fdd/PQQw9x6aWX8vTTT3P99ddz0003sWzZMlpbW0kkEtx88828613v4qtf/SqZTIb29vYhLeq+qOYsqDmbWZZSciYiIiIiMoDCpo2FTRrvuusujjvuOBYvXszzzz/fowlib48++ijve9/7KC8vp7q6mve+973d7z333HOccsopLFiwgNtvv53nn39+r/Fs2LCB2bNnc+SRRwJw2WWXsXr16u73zz//fACOP/54Nm3atNdpPfbYY3zoQx8C4IwzzmDXrl00NzezbNkyrrrqKm688UYaGxuJRCIsXbqUW2+9lWuvvZZnn32WqqqqvU57MFRzFtScTSvr4oVWJWciIiIicoDYSw3XSDr33HP53Oc+x7p162hvb+f444/n1Vdf5frrr+fJJ5+krq6OFStWkEzuW0/oK1as4O677+bYY4/ltttu4+GHH96veOPxOADhcJh0et9unXX11Vdz9tlnc99997Fs2TJ+97vfsXz5clavXs29997LihUruOqqq7j00kv3K1bVnAU1Z1Pinao5ExEREREZQGVlJaeffjqXX355d61Zc3MzFRUV1NTU8Oabb3L//ffvdRrLly/n7rvvpqOjg5aWFn796193v9fS0sLUqVNJpVLcfvvt3eOrqqpoaWnZY1pz585l06ZNvPzyywD85Cc/4dRTT92n33bKKad0z/Phhx9mwoQJVFdXs3HjRhYsWMCXvvQlli5dyvr169m8eTOTJ0/miiuu4KMf/Sjr1q3bp3kWUs1ZogaAidFO6ncrORMRERERGcjFF1/M+973vu7mjcceeyyLFy9m3rx5zJw5k2XLlu31+8cddxwf/OAHOfbYY5k0aRJLly7tfu8b3/gGJ554IhMnTuTEE0/sTsguuugirrjiCm688cbujkAAEokEt956Kx/4wAdIp9MsXbqUj3/84/v0u6699louv/xyFi5cSHl5OStXrgT87QJWrVpFKBTimGOO4ayzzuKOO+7gW9/6FtFolMrKSn784x/v0zwLmXNuvycyWEuWLHG5exyMGpk0fGM8j8+8kktePo2//vNZRMOqUBQRERGR0efFF1/kqKOOKnUYMkh9rS8zW+ucW9LX55WFhCMQraA2nMQ52N3WVeqIRERERERkDFJyBpCoptp815e67kxEREREREph0MmZmYXN7Ckz+03weraZPWFmL5vZnWYWG7kwR1i8mgqn5ExEREREREpnKDVnnwFeLHj9r8C3nXOHAw3AR4YzsKJKVFOWbQWUnImIiIjI6FbMPiNk3+3LehpUcmZmM4CzgR8Grw04A8h1k7ISOG/Icx8t4tXE0kFypnudiYiIiMgolUgk2LVrlxK0Uc45x65du0gkEkP63mC70r8B+CKQu+31eKDROZe7i9sWYHpfXzSzK4ErAQ455JAhBVc0iWpCja9RnYio5kxERERERq0ZM2awZcsW6uvrSx2KDCCRSDBjxowhfWfA5MzMzgF2OOfWmtlpQw3KOXczcDP4rvSH+v2iiFdDZzMTq+LsaNm3O5mLiIiIiIy0aDTK7NmzSx2GjJDB1JwtA95rZu8GEkA18B9ArZlFgtqzGcDWkQtzhCWqIdnMxElx1ZyJiIiIiEhJDHjNmXPuy865Gc65WcBFwEPOuUuAVcAFwccuA341YlGOtHgNpDuYUhlWciYiIiIiIiWxP/c5+xJwlZm9jL8G7ZbhCakEEtUAzEiklJyJiIiIiEhJDLZDEACccw8DDwfPXwFOGP6QSiDuk7OpiS7aujK0daapiA9p0YiIiIiIiOyX/ak5O3gENWeT410A7FR3+iIiIiIiUmRKzgASNQBMjPqkTE0bRURERESk2JScQXezxnHhDkDJmYiIiIiIFJ+SM+hu1lgT8vc4q1ezRhERERERKTIlZ9Bdc1ZJGyFTzZmIiIiIiBSfkjPoTs5CnS2Mr9SNqEVEREREpPiUnAGEIxCtgM5mJio5ExERERGRElBylpOohmQTk6rjuuZMRERERESKTslZTry6u+ZsR7OSMxERERERKS4lZzlBzdnEqjg7WzvJZl2pIxIRERERkTFEyVlOvBqSzUysipPOOho7UqWOSERERERExhAlZzmJoFljVRxQd/oiIiIiIlJcSs5ycjVnlUrORERERESk+JSc5SRqetactSZLHJCIiIiIiIwlSs5yEtWQTjKx3ADVnImIiIiISHEpOcuJ1wBQ6dpJRENKzkREREREpKiUnOUkqgGwoGmjkjMRERERESkmJWc5cZ+c0dnMhMo49a1KzkREREREpHiUnOUENWckmxlXHqOhTfc5ExERERGR4lFyllNQc1ZbHqOxvau08YiIiIiIyJii5CynoOasrjxKQ7tqzkREREREpHgGTM7MLGFmfzazv5jZ82b29WD8bDN7wsxeNrM7zSw28uGOoB41Z1E6UhmSqUxpYxIRERERkTFjMDVnncAZzrljgUXAmWZ2EvCvwLedc4cDDcBHRi7MIsglZ8kmast9ntmo2jMRERERESmSAZMz57UGL6PBwwFnAD8Pxq8EzhuRCIslHIFoRdCsMUjOOnTdmYiIiIiIFMegrjkzs7CZPQ3sAP4AbAQanXPp4CNbgOn9fPdKM1tjZmvq6+uHI+aRk6iGzibqyqMA6rFRRERERESKZlDJmXMu45xbBMwATgDmDXYGzrmbnXNLnHNLJk6cuI9hFkmiBpLNBc0aVXMmIiIiIiLFMaTeGp1zjcAq4C1ArZlFgrdmAFuHObbii1dDZzN1FUHNma45ExERERGRIhlMb40Tzaw2eF4GvAN4EZ+kXRB87DLgVyMVZNEkqntcc9agmjMRERERESmSyMAfYSqw0szC+GTuLufcb8zsBeAOM/tn4CnglhGMszji1dCwiUQ0TCIaUrNGEREREREpmgGTM+fcM8DiPsa/gr/+7OAR1JwB1JXH1KxRRERERESKZkjXnB30gmvOAGrKoqo5ExERERGRolFyVihRDekkpLtUcyYiIiIiIkWl5KxQvMYPgx4bVXMmIiIiIiLFouSsUKLaD5NN1JbHaFTNmYiIiIiIFImSs0LxfHJWVx6lsSOFc660MYmIiIiIyJig5KxQruas09/rLJN1NCfTpY1JRERERETGBCVnhbprzpqpDW5ErevORERERESkGJScFUoUdAhSHgVQj40iIiIiIlIUSs4KJfasOWtQzZmIiIiIiBSBkrNC8fw1Z7VBzZmaNYqIiIiISDEoOSsUCkOsEpK+QxCAhrYhNmvcuhbW3gbq5VFERERERIYgUuoARp14NXQ2UVMWxQwaOwaZnHW2wEPXwRP/D3Bw6DKYcMSIhioiIiIiIgcP1Zz1lqiGZDPhkFGdiA6uWeOG38JNJ/nEbP77/bhXHxnZOEVERERE5KCi5Ky3eDV0NgNQVx7de2+NbbvgZyvgvz4I8Sr4yO/h/T+E6hnw6urixCsiIiIiIgcFNWvsLVEN7bsBqC2P7b3m7KF/ghd/A2f8A5z8GYj469SYcypsuB+yWQgp/xURERERkYEpc+htj5qzfpIz5+ClB+DId8HyL+QTM4DZy6FjN7z5XBECFhERERGRg4GSs94S1ZBsAqCuPNZ/b427NkLzFjjs9D3fm73cD9W0UUREREREBknJWW9x3yEIDNCs8ZVVfjinj+SsehqMP0KdgoiIiIiIyKApOestUQ2ZTkh3Ulcepa0rQ1c6u+fnNq6C2kNg3Jy+pzN7OWx+HDJDvE+aiIiIiIiMSUrOeovX+GGymdryKMCetWeZNGx61NeamfU9ndnLoasVtj01gsGKiIiIiMjBQslZb4kgOetsprbcd/Kxx42ot671nYb0db1ZzqxT/PAVNW0UEREREZGBDZicmdlMM1tlZi+Y2fNm9plg/Dgz+4OZvRQM60Y+3CJIVPthsom6IDlraOtVc/bKKsBg9qn9T6diPExZoOvORERERERkUAZTc5YGPu+cOxo4CfikmR0NXA086Jw7AngweH3giwfJWWe+WeMeN6LeuAqmLYLycXuf1uxT4fU/Q6pjBAIVEREREZGDyYDJmXNuu3NuXfC8BXgRmA6cC6wMPrYSOG+kgiyq7pqzZuoqgmaNhdecJZthy5N999LY2+zlvnOR1/88AoGKiIiIiMjBZEjXnJnZLGAx8AQw2Tm3PXjrDWByP9+50szWmNma+vr6/Qi1SApqzur6qjnb9Bi4zN6vN8s59GSwsJo2ioiIiIjIgAadnJlZJfAL4LPOuebC95xzDnB9fc85d7NzbolzbsnEiRP3K9iiKKg5K4uGiUVCPWvOXlkF0XKYeeLA04pXwfTjdTNqEREREREZ0KCSMzOL4hOz251z/x2MftPMpgbvTwV2jEyIRVZQc2Zm1JVHaShMzjau8jVikfjgpjd7OWxd131jaxERERERkb5EBvqAmRlwC/Cic+7/FLx1D3AZ8M1g+KsRibDYQmGIVXYnU3XlsXyzxqYtsOslOH7F4Kc351R49Hp/Q+q5Z+bHv/EcrPsxuKxP9KJlfpiohQUfgLLa4ftNIiIiIiIy6g2YnAHLgA8Bz5rZ08G4r+CTsrvM7CPAZuDCkQmxBOLVkGwCoKYsmm/WuHGVHw7merOcGSdAOO6bNs49Expfh1XXwV/ugEjCJ2XpZNCjY9Ay9KmfwqV3Q9nBcXcCEREREREZ2IDJmXPuMcD6efttwxvOKJGohk6fnNWVx9hY3+rHv7IKKifDpKMHP61oAg45EV5+AH4fgidu9uNP/hScclU+AXMOMl3w8oPws8vgJ++DD92tGjQRERERkTFiSL01jhl1s2HD/fDbrzA1kfTNGrNZeOVhmHMaWH+5aj9mnwo7N8Dj34H574dPrYV3fqNnzZiZb9Y4791w4U98s8efvA86Gofxh4mIiIiIyGg1mGaNY8+534GHvgF/+i5fiNxOKvl+3PZxWPuuwd3frLfFH4K2nbD4f8GU+QN/fu6Z8MGfwJ0fgp+eDx/6JSRqhj5fERERERE5YJjvBb84lixZ4tasWVO0+e237X9h252fZVrjOly8CutsgavWQ/XU4sx//X1w16UwdSG87/sw/vCh19qJiIiIiMioYWZrnXNL+npPzRr3ZuqxPLZsJX/b9WkysWrfuUexEjMImjiuhO3PwHeWwL/Nhts/AI/8m++cJJUsXiwiIiIiIjKi1KxxAHUVce7LnsTHP/AZFk6rKn4A886Gv3sSNj0Kr/8ZtqyBl/4AOKg5BN7xdTjmfapRExERERE5wCk5G0BdeRSAhqQb/I2nh9u42f5x3KX+dbIJNj0Gq/43/PzD8OcfwJn/AtMWlSY+ERERERHZb2rWOIDa8hhA/l5no0GixteofWw1nHMD7Pwr3Hwa3P1JaK0vdXQiIiIiIrIPlJwNoLvmrG0UJWc5oTAs+TB8eh2c/HfwzJ1w+/shmyl1ZCIiIiIiMkRKzgZQU+aTs8aOVIkj2YtEDbzzn+F9/w+2/wXW3lrqiEREREREZIiUnA0gEg5RlYjQ2D6Kk7Oc+e+HWafAg9+Atl2ljkZERERERIZAydkg1JXHaBhN15z1xwze/S3oaoUHry11NCIiIiIiMgRKzgahrjxKw4FQcwYw6Sg48eOw7ie+230RERERETkgKDkbhNry2OjqrXEgp10NlZPh3s+rcxARERERkQOEkrNB8DVnB1ByFq/yHYRsfxrWrSx1NCIiIiIiMghKzgahtjxGY9sB0qwxZ8EFcOhb4cF/gvbdpY5GREREREQGoORsEOrKY3oOhH8AACAASURBVLR0pkllsqUOZfBynYMkm+G+L6h5o4iIiIjIKKfkbBDqKoJ7nR0onYLkTD4aTv0SPPdzuOtS6GovdUQiIiIiItIPJWeDkLsRdVPHAXTdWc5pX4Iz/xXW3wu3nQ2tO0odkYiIiIiI9EHJ2SDUlccADpzu9Hs76eNw0e2w40X44dugfkOpIxIRERERkV4ipQ7gQNCdnLUdgDVnOfPOhg/fC/95EdzyDnjb16CrDRpfCx6vQ6wCLrgF6maVOloRERERkTFHNWeDUFt+gF5z1tv04+GjD0DlFH8PtD98DZ79ObRsh3FzYNfLcOu7YefLpY5URERERGTMGbDmzMx+BJwD7HDOzQ/GjQPuBGYBm4ALnXMNIxdmadVV5Jo1HsA1Zzl1h8LHVkPDq1A9DRI1+ffeeBZ+fB7cehZc+ivfoYiIiIiIiBTFYGrObgPO7DXuauBB59wRwIPB64NWRSxMNGwH7jVnvUUTMOmonokZwJQF8OH7wEK+85DtfylNfCIiIiIiY9CAyZlzbjXQ+y7G5wIrg+crgfOGOa5Rxcz8jagPhpqzgUyc6xO0WAXc9h54/clSRyQiIiIiMibs6zVnk51z24PnbwCT+/ugmV1pZmvMbE19ff0+zq706sqjB0ezxsEYf5hP0MrHwcpz4Nefge3PlDoqEREREZGD2n53COKcc4Dby/s3O+eWOOeWTJw4cX9nVzK15bGDp1njYNQeApf/FhZcAH+5E75/CtzyTv88lSx1dCIiIiIiB5197Ur/TTOb6pzbbmZTgYP+zsYTK+M88epuWjvTVMbHyB0IqqbAuTfBO/8Znv4vePKH8Msr4f4vwrTFMPkYf+3apKNh4jyIlZc6YhERERGRA9a+1pzdA1wWPL8M+NXwhDN6fXjZLHa3dfL1e54vdSjFV1YHb/lb+Ls18KG74ahzoKPBJ2u/+iT84HT45kxYfT24fitRRURERERkLwbTlf5/AacBE8xsC3AN8E3gLjP7CLAZuHAkgxwNlswax9+edjjfWfUyZ8ybxFkLppY6pOILheCw0/0DIJuBhk3w5vPw3M/hoW/AG8/Aud+FeGVJQxUREREROdCYK2JNx5IlS9yaNWuKNr/hlspkueB7j7NpVzu/++xyptQkSh3S6OEc/M93/I2tJx4FF90O42aXOioRERERkVHFzNY655b09d5+dwgylkTDIb79wUV0pbN8/mdPk82qCV83Mzj5U3DJz6F5K9x8Gmx8KP9+Ngtd7dC+W00fRURERET6MEZ6thg+cyZW8rX3HM2X//tZfvTHV/noKXNKHdLocvjb4MpVcMcl8NP3+xtdpzogXdDD45zT4f0/hIoJpYtTRERERGSUUXK2Dy5aOpNV63fwb7/dwMmHTeDoadWlDml0GTcHPvIH+OMNkGyCSAKi5RBNQFcb/PFG+P6pcOFKmNFnja6IiIiIyJija8720e62Ls68YTUV8QgrP3wCh4xXN/KDtu1puOtD0LwdzvwXWPpR3yxSREREROQgp2vORsC4ihg3XXIcu9u6eM93HuPRl+pLHdKBY9oiuPIROOwMuO/v4b+vgM6WUkclIiIiIlJSqjnbT5t3tXHlj9fy0o4Wrj5rHlecMgdTLdDgZLPw2L/DQ9dBKOKbOM56q3/MOEE3tRYRERGRg87eas6UnA2Dts40f/+zv3D/c29w7qJpfPP8hZTFwqUO68CxZQ28eA9sesw3eXQZCEWhdiaE4xCJQTh4lNXBhCMLHkdAQtf8iYiIiMiBQclZETjn+O7DG7n+9xuYO7mKz73jSN5x1GRCIdWiDUlnC7z2BGx6FJq2QKYTMilIB8O2HbD7Fcim89+ZcCSc/Gk49iIIR0sXu4iIiIjIAJScFdGq9Tv4x189x5aGDuZMrOBjy+dw3uLpxCOqSRs2mRQ0bIL6DbBzA7zwK9j+F6g9BE75PBz7N762TURERERklFFyVmTpTJb7nnuD7z+ykee3NTOpKs5lJ8/ilCMmcNTUaqJh9cMyrJyDl34PD38Ttq2Dmpm+Ju2Y86ByUv/f62r3n0825e/FlurwtXKHvwMmHF683yAiIiIiY4KSsxJxzvHYyzv5/iOv8NjLOwGIR0IsnFHD4kPqOO6QOk4+fDzVCTXFGxbOwcsPwiPfhC1PAgYzT4C5Z8Hcs/31afUb4OUH/GPz477ZZJ8M5p3tk7xDTizmrxARERGRg5iSs1FgW2MH615r4KnXGln3WgPPb22mK5MlEjJOmjOetx01ibcfNZmZ49RD4X5zDt54Fjbc5x/b/+LHx2ugs8k/nzgPDn87zDkNKiZCtMw/ImU+YVu7Ep78AXQ0+J4jl30ajjwLwrpvu4iIiIjsOyVno1BnOsNfXm/iwfVv8sALb7Kxvg2AuZOreMth4zlx9jiWzh7HhMp4iSM9CDRtgQ33w/anYcZSOOxtvifIgXS1wVO3w/98Bxo3Q6wKZi2DWafA7OUweT6E+mmi2tHoOy7Z/QrsftUndbOWw9RjleCJiIiIjGFKzg4Am3a28cCLb/LQ+h2se62BZCoLwJwJFSydNY5Fh9SyYHoNR06uIhbRNWtFlUnDX3/rm0K+uhp2b/Tjy+qgahrgfG1dbthWDx27+55WvNrfx232cph+PJSNg7JaSNSop0kRERGRMUDJ2QGmK53luW1N/PnV3Tz56m7WbG6gqSMFQCwcYu6UKuZPr2HB9BrmT69m7pQq9QZZTE1bfVf/mx71NWQAFgIzwHyyNe4wGDfHP+pm+Vq4TavhlUd8gtfw6p7TjVX6JpZT5sOUY2HqQl/TVjVl4JgyaWh6zd+KIJWEVLvv4CSdhAlzfTPO/mr5hsIFCehwTEtERERkDFJydoDLZh2v7W7n2a1NPLetiee2NvHsliaak/5eX9GwceTkKhZMr+GQ8eXUlEV7PMZXxplcFSeiXiJHj4bNUL/eJ3fJxvyweau/Xm73K/nPlo/3PVDWzIDqaf5RPt5PY+cG2PkS7NoI2VT/8ysbl2+Seegyf9uBSMLX1tkg7sVX/1d49i549mfQvN0njtOO87V/04/zyagSNhEZLZLNvrXD+nt9k/aZJ/nOoQ47HWIVQ5hOE0Qr9t4cveUNeOkPEI7B3DN9SwgRkb1QcnYQcs7x+u6Ongnb1iYa2/s+QQ+HjCnVCabXljGtNsHk6gTVZVGqEhH/iEcpi4XJZB2ZrCOddWSyWcyMydUJptUmmFAR1021iyXZ7JO0N56BHS9A8zZfY9e8Ld+piYVh3Gx/E+4JR/reKBO1EE1AtNwnX6GIn87mP/qavsbXes7HQv5zkYRP+OpmBY9D/bBhEzxzl4/DQr455qSjYdvT/oQn1e6nE6+BaYvyydr0430S2dnqE86mLf7RVu/nFSv3NYXRch9vutPf2qCr1U+zq92fQJWPDx51fhit8AllJA7heD4hdA5c1t8GIZPyNYadLX56na1+6Jxvipp7JGoGvv4vkw5i6giWV1A7asF80x3+va42P0x1AC74DMHQ/HTSSch0BTdU7/TvhWPBI+qHZvnfkntE4n69dsde69dr+25o3wXtO/0w3eWvpaw9FConD0+ynO6Czubg0eIfoQjEq/KPWFXfy7GrHVrfgJY380OAiUfCxKN8jXBhwUDzdtj2lN+umrfB5GNg2mKYsmBoJ9Ol1tEIO//qe4atX+9P3GtnwvgjYPzhfj8tH1fqKA8umbTfxpq3wZvPwfr74NVH/P5WPsEfk15/wida4TjMORWOeKdfH5WT/bZYVue3x5Y3guPlH/2wfr0/7kw/zvf+O+MEf+1y0+u+uftff+u325xwzHc2dcz5PlGLVw3uNzgHLduD7SbYdhpf8/tbJObjjiR8x1XjD/OtISYd7W8Xk9uPslm/HBo2+/jSnT2PWWZ+edTN8ttkZJDXtCebfAFd+06/L8Yqg32/0h9HY8PQkVkmDa1v+ke8GqqnHlj7vRSPc37fHuz2O0opORsjnHMkU1maOlI9HvUtnWxr7GBbYwdbguGOlk660tkhTT8WDjGlJsGU6gSViQhlsTDl0TDlsTBlsQjlsdzzYBjtPS6Sfx4NqyZvX3W2+JPxqqlDPzg1bIbX/gfadvrEIt0Z3N8t6f8UGzb5z+QSQPAnyAsuhPnn92ximUn7mrut62DrWv/Y8YJPkMAnXrnkbaSEgqQgN8+hipZDKOqTi1A0uO7PINXmk7p+b7UwyoXjvna0eqo/Yct0+d+SSfnn3cf9YOhckNh2BY+U3zb2VhtbyPrYl90Ax5dEjT/BjFf75L/1zfy0ErX56zYtFJyIHuXjyiWJXa0+KQ5Fgp5Wg0KGSDyf5OauA3XBMui+n2HSb/8W8p+PlPlhtMyPy2b88sim/fNQyBdAJGogUe2HoYg/aU02+mFHI7Tu8CfHheuhaopPGgqXZaLWJ2iJGv/7c0MAl8nP12WCWIJx3e9lC+JL94q34LVZvhfaaJkvCLFwsN93BMsh6ZdTxSR/op9LVhI1vpCooyH/SDb6ddAdS/CIlQfJenVw0l7hp5tsKng0+/2tYoKfT8UkqJzol1E27ZdPJogf55dvKOzjDUX8+M5mP53csGO3L7RqfaPn9lY3C+ad4x8zT/DTyaT8sW/D/b42rXFzz+0xFPUFH231/nWsEg45yde4tdXDlj/D9mf8785v+DBjCRx5pn+k2uH5X/pHy3a/PU46Ol9olNu3uo9XBedfXW3+dxVuI+Nm+9+VDvbfdFew/Rccn8vqfJLZ0QCNrw/hmGX+P6Rull8n0XK/HqPlfv0lm3yCWL/B/5a9iVVB1WSonOLXbfl4v85yCWEueUx3Bc3tgwKqrna/37ds9/sOvc5HEzX+uu7qqX57KR/v95tcoZ2FgkKxtl6FZO35Qr5Uu1/moXBBPMFlIL33L1xBgVnMHxNCkYJ1V3AM7Us4lj8ORRN+2+5s8dtPW73/322r99OsyBU8TvDDeFVQ6BjLD7PpgkK4XX57T3X02qeDY5eFC/aXkB9aqGBc4etQz89bLnkvuDyjszk/747d0N7g/yfL6goKC2uDOAuOVS7jvx9JFBQqxIN9MN3rOFXwyKQKjmOp/HEsk/LLvaPRx9Ox2w8znX67q54GNdOherp/Hor2OlZmfIuhI985yP2ieJScSZ860xlakungkaKjK0MkbIRDISIhIxwyMlnHG01JtjV1sK0xybbGDt5oTtLelaa9K0NHV6Z72JUZerKXS+RyN+bOHcMNX9sXj4SJR0PEIyH/PBIiHg2GwbhoxIiGQkTCRjQcIhwyQgapjKMrnaUrk6UrnSUVDAvHpbOORDREeSxCRZBklkXDtCRT7G7vYndbFw1tXTS0p6gtj3Lo+AoOHVfOoePLOXR8BTVlUcLBsoqEjFBuaPnX4T7GDYZzjs50ls5UlnDYiIb97yz8fjbr6Mr435HOZEkEy8YG01RxbzoafKIWr/altIOV6vA1dVvX+evqKicHTTKDg2flZP/H1tUW/HkGtVKRRFAiGzwiZf697oPxLv881V5Q+5TKn4iEIvmTuVAkmF4lxCvzpbxY0IS0wR/oOxr8H1D3H0NwcogriCX4fjThv++y5DuAIV8LGC0P/ijL/ed6dxITjhX8UQVDyJ+s5f74nSv4kwz+KNPJguavwUlyJp0/UakI/txDUV9a3rDJPxo3+9qqXK1cLoZQtGcyldtWCmvwcs9jFT4pKawpy6Z7JkjJ5r6TuGiZP1mrCh6VU/wfZv162LEe6l/0w2STrx2btqhnTVnzdl+Ltu0p/9j5V7+8C2OJVvh5pwoKGtLJYDn2qjEIR/M1yrkTGufyiUq6M1/zmUsIcttUNt0zKUg2+fkmcglb0KlPxQRfiz1xrn/UHpo/KWncDLte9s2QG14Nkp3cNJv88oSeCUnhiVVu2y58rzvGfl67bEFC2u6TsWw6n8xGE35fc1l/0piruehoyK/HWGX+RCxR65dj4TwslK/17mzJ17JGygqWT7ANpTqgbYc/EW+r30vhTbAP9RaKBMlstR+W1RU09w6OMeNm+2Slj2Ogc47WzjS7Wjpp3/EKFZ1vUJnaTVlnPfHkTsIdu/x6O3QZTFm4Z41wV5vfFres8ev6iHf5BLO3bBZe/5NP0na97Pf3cDS/f4XCwW8s2P8iiYJtZ56//riv43iu06kdL/j9Z8cLvhl8WV2+1UPtLF84E03sWUjRusNvi7mCuMbN/vhamMyk2v2+lYsltz1XTgqSyNb8+k7mCiXeDGrJ3/TTc9n8PHPzj8TzBSi555WTfJJYPc0PKyf56TZv9ceAlu3+eVuQoKTa+tlmcptOyMceyx2TgyapuXiymXxM3ftTOL9Ouo/JQTKcTfVK2HLH0F7rJlebU3g8SSf9dloxwa/Pion+WJ3N5BOu9p3+t3W1Bt9P9ixsjFX1TEhzLU26k9CgoMll/XZXmCS5bP737jEuw15ZKNjvg3mX1fllkfvvzF2OkZtOYQLo3OALCSxUcDyJ9jyOhXPDWFCgFbSiKRvnl2v7Tt8ip3mb30YKE/zuBDQCJ38Kzvjq4OIpIiVnUhTpTJb2VD5ha+9KFzzP7JHQtafy72eyjty26Aj27WyQnKQzwTBLZypDV+55OkNnyj9PZ7Nk+9mUo2EjFg4RjYT8MOwTu2jYJ3TJIOa2giSzLBpmXEWs+1FbHmV3Wxebd7WztbGDTH8zG6Rc8puLKxoklmbQ0ZWloytNeypDX7tnuCBx7iuOSMioiEeojEeoiIcJmfn/SFww9CcpLljYha97fM75ZVce89Mqj4epiEUIh4z2rgzJVH6ddqWzOCDrXPc0QmaURXvWpJbF8h3XdMcAJCJhymI+Sc59J5fQhsz/5lDISGccqYxf57mEO5NbSAWLIhSyXgm9L3AwCv5Q+37qX5v/ZK7Q17B8wUHwpPv94L3c66zzyyHrfPKcyTo6UhnaOtO0daZp6UzT3pkhlcnuseyj4RBVCb/eKuNRKuNhYpEQ2WB/cM5PLxwOUVsWpa7cb5u15VHKYxG60lmSqQzJYN/oymQJGYTMgkILK3jul1M4GBcKscdnem8X+e2l5/bUezmaFey/QRydKb+vN3Z00dieorHd1+x3pjPUlMWoK8//nqpEtMc2klvFsUiIWHfBjH8eMuu5HgzikTBl0TDRsO1XQUVueefWqetet36cc65gffsgY2EfVywc6tE6wDkXFKI4Ms4Fx6L9i2/EpDv9CXK82p+MFsj9Zr9c/G/e5wKhzlZ/whcKEr5wcHIGBSeaQem3hYKazb6Trs50lo6uDI0dKV7f3c5ru9u7h1saOtjZ2smutq69thjJbVOx4L8hEvLrCOixD2YdlMXC1JT5fa+uPEZNWZTKeIRENNRdqJjoLlzMFzImov6Y3N6VDo6h/pHOOCrivoWJ3//9sTA379z2B1CViFCd8JcljMilBrkZjcZtM9WRr9HB+eQrWpYvJMs1Dz+QZTN+HwyFR7bpXm4fK2xKT/A8WjFw0/i9dRDmXK8kN92rYCf3GMYWVJkgqQ2FD4htYG/JmW64JMMmEg5RHQ5RnShNl/DZ4Fq5XKLWV03TYKQz2b02uUxlsmxt6GDz7nbaOtOks6573rlhxjkymSwZB5lslky219C5INHwyUYq409gnYNE0FQ01wQ0HgmTyWa7P5vOOFLZLNFQPsGMBTWGhUlAa6d/nnVuj5NXC66J8glFQSJS8BqDdMbR3pWmrTPD7rYuXt/dTjrrKOuOMcL4yjjxHifJfhpZ5+joytARnIDsbusgmfKlbN1rJHjSmcoGn0t330ZiMHJJbvfkgqeZrF+2o40ZVAQnX7lkvDC5S2UcLckUbUGBxcGuOhEhFgnT1NE1IusrHPIFBImoTx56JFPZwmSr7+Rrf4XMHxf7K0jxiWSou8Y71L0vWvf7exYA5PdPg+79zjnIBL/LD/15T6JXohANh7p/J+QLwnzBWpqOoLAqmcp2J2Dd0y5IEHov58Lrl6sSkSC575XE91NIlH/fTzyVcb6QIZUhGRQ4pLOOaMiIBEltJBTC4bqTm76WbywSYmZdGTPqypk7pYrxlTHGV8QYXxGntjxKZzpLa2ea1mTumJnuLuxLZ4JWCcF2mSvMyBUYdaSyNLZ3sau1i431rTS2pWjrSg/LdjNYZlAZ94laJBwUtOQKXoIClnDIFwCEg0Iu/zw3Pl8gA/kCptyBubDQyTBSmSwtyTTNyZQfdqToSPkWN9GQL2iMhHxBYzScW1e59ZWPJRdfbtsNFYwzg45UhtakXx+59ePwx/tI8F8XDRnxaJiyaHOPyynikXB3PLnWNCGz7u0O+imMpKC1d0EBZe/3exZS5QsY+y743HM64H9nLjb/m/z23GNcsKxyfQBkg3OGrHP5wrWC9R22YN0Gr83yBYp73Yb2KJrsezsbeDoD67mseh5LXMHxqPfneo8nWMa542puGTnniEVCQQFvhLKob511zLQa5k8/sDrp2a/kzMzOBP4DCAM/dM59c1iiEtkHoZARCxkx9q8kZqBr4aLhELMmVDBrgi5WHgnZrCOZ9idjLutPCnMl1uGQEQtqPWPhvSfeuSafvnY1Q6rgrKmwxUBfJ5v91TTu8efd+7NBIWLujzJX61cWDVMRlIQPprAgd/1oa2earkw2qN3K13SlMlkaO3ztU0N7F03tKdq70t1NfnMn+7lat1wykjvZLqz5yDpHJuuXV9a5/Al+1nX/wfvzNetxouZPqvJ/7oV/ouBPHAqbJMci/o+yrjxGbVmU6qBJcO73tnVlaGjroqkjRXMyVTCPfPLha00zBbVxWZ8wFKyHrKO7BjFXONAR1EKHCk4EzfIn2qFQXyeJPU9y9/Z+bnoOSBU0m+5KZ0lls8HJVqj75NI3u872aBGQ7OO39F2zvecJYC4RKzzxDgUFJJ1pP+1kMI+WZHqPAplwyKgpizK1OkFZd6FQKF973c8JYG5bbu/M9Dhhb0mm6Upn9ywICoER6rdGOrdnRMNGIhoOHj6xDId9zXm6uxm339ByNfq5YVUiwsxx5cysK2dSVfE7scqt12Qq02Pd5tZDZzpD1rnua7LLYr6WNxL2LRJ84ZofdqQy+X0gWEa5ppnNybTfV4L9JVO4f2fzibrfpwv276AAsytDj+S78BgG+e0M8uPCIaO6zC/fXM1driOxrj6S2nSvQsXCgo9MUIjasxbax14WC1NbHmPGuHIqYxEq4hFCRtBRWX66vrbUt95oSaZ5szkZtKhw3bGkMn4+hQUahce1wn2Bwu2RvgtGyI3v9R4F0+q74DM/7VyilcpmyWQcqeCyhO5xBYWL4VB+n474A2534UomO3DBycHKjD0uFwkF/40dvVod/e1ph42d5MzMwsBNwDuALcCTZnaPc+6F4QpORMaeUMg3pRyO6SRC/gQPDqwbfJtZ90lyfyZVJ4oY0cgyMyqDprgzSx2MyH6IBrVFlXE1TJJ9l0uMh9JcuK8mx/sfxyA+09f1oX1Mp7uwBgoKZYLhHolx79rcwS+LXDPn9qBwLtc8+ECyP0ePE4CXnXOvAJjZHcC5gJIzEREREZF9sC/XcBY2Xx3LzPI17weq/Wn/NR14veD1lmBcD2Z2pZmtMbM19fX1+zE7ERERERGRg9eI32jKOXezc26Jc27JxIl9dDkrIiIiIiIi+5WcbYUelwfMCMaJiIiIiIjIEO1PcvYkcISZzTazGHARcM/whCUiIiIiIjK27HOHIM65tJn9HfA7fFf6P3LOPT9skYmIiIiIiIwh5op4YwQzqwc2F22GgzcB2FnqIGQPWi+jk9bL6KT1MjppvYxeWjejk9bL6KT1MrwOdc712RlHUZOz0crM1jjnlpQ6DulJ62V00noZnbReRietl9FL62Z00noZnbReimfEe2sUERERERGRgSk5ExERERERGQWUnHk3lzoA6ZPWy+ik9TI6ab2MTlovo5fWzeik9TI6ab0Uia45ExERERERGQVUcyYiIiIiIjIKjOnkzMzONLMNZvaymV1d6njGKjObaWarzOwFM3vezD4TjB9nZn8ws5eCYV2pYx2LzCxsZk+Z2W+C17PN7Ilgv7kzuAm9FJmZ1ZrZz81svZm9aGZv0T5Temb2ueA49pyZ/ZeZJbTPFJ+Z/cjMdpjZcwXj+tw/zLsxWD/PmNlxpYv84NbPevlWcBx7xsx+aWa1Be99OVgvG8zsXaWJemzoa90UvPd5M3NmNiF4rX1mBI3Z5MzMwsBNwFnA0cDFZnZ0aaMas9LA551zRwMnAZ8M1sXVwIPOuSOAB4PXUnyfAV4seP2vwLedc4cDDcBHShKV/AfwW+fcPOBY/DrSPlNCZjYd+DSwxDk3HwgDF6F9phRuA87sNa6//eMs4IjgcSXwvSLFOBbdxp7r5Q/AfOfcQuCvwJcBgvOAi4Bjgu98Nzh3k5FxG3uuG8xsJvBO4LWC0dpnRtCYTc6AE4CXnXOvOOe6gDuAc0sc05jknNvunFsXPG/Bn2ROx6+PlcHHVgLnlSbCscvMZgBnAz8MXhtwBvDz4CNaLyVgZjXAcuAWAOdcl3OuEe0zo0EEKDOzCFAObEf7TNE551YDu3uN7m//OBf4sfP+BNSa2dTiRDq29LVenHO/d86lg5d/AmYEz88F7nDOdTrnXgVexp+7yQjoZ58B+DbwRaCwkwrtMyNoLCdn04HXC15vCcZJCZnZLGAx8AQw2Tm3PXjrDWByicIay27AH5SzwevxQGPBH6n2m9KYDdQDtwZNTn9oZhVonykp59xW4Hp8CfN2oAlYi/aZ0aK//UPnA6PH5cD9wXOtlxIzs3OBrc65v/R6S+tmBI3l5ExGGTOrBH4BfNY511z4nvPdiqpr0SIys3OAHc65taWORfYQAY4DvuecWwy00asJo/aZ4guuYToXnzxPAyroo5mQlJ72j9HHzL6Kv8zh9lLHImBm5cBXgK+VOpaxZiwnZ1uBmQWvZwTjpATMLIpPzG53ySUemQAAAlRJREFUzv13MPrNXDV5MNxRqvjGqGXAe81sE77Z7xn465xqgyZboP2mVLYAW5xzTwSvf45P1rTPlNbbgVedc/XOuRTw3/j9SPvM6NDf/qHzgRIzsxXAOcAlLn+PJ62X0joMX9D0l+A8YAawzsymoHUzosZycvYkcETQi1YMf9HpPSWOaUwKrmO6BXjROfd/Ct66B7gseH4Z8KtixzaWOee+7Jyb4Zybhd8/HnLOXQKsAi4IPqb1UgLOuTeA181sbjDqbcALaJ8ptdeAk8ysPDiu5daL9pnRob/94x7g0qAHupOApoLmjzLCzOxMfPP59zrn2gveuge4yMziZjYb3/nEn0sR41jknHvWOTfJOTcrOA/YAhwX/P9onxlBY/om1Gb2bvw1NWHgR86560oc0phkZm8FHgWeJX9t01fw153dBRwC/7+dO0SJKIrCAPzfol1cgStwCbMAsxbLgMEFWMQg7sFqttrcgkGYHYhGQasgCMdwJ4iMzXEuzvfVd3lcuJzwv3fOzVOS/apaNKzKkrXWJklOqmqvtbaT/idtK8ksyWFVva9yf+uotbabflHLRpKHJNP0D25qZoVaaxdJDtLbs2ZJjtJnMdTMH2qtXSeZJNlO8pzkPMlNFtTHPEhfpregviWZVtX9Kvb93/1wLqdJNpO8zpfdVdXxfP1Z+hzaR/rIw+33d/I7Fp1NVV19ef6YfhPti5pZrrUOZwAAAKNY57ZGAACAYQhnAAAAAxDOAAAABiCcAQAADEA4AwAAGIBwBgAAMADhDAAAYADCGQAAwAA+AVSTXtOXLFiZAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    # This is a neural network that predicts a structure of SH2 attached to a peptide.\n",
        "    # Pay attention to create inputs and labels to the network using the code in the utils script.\n",
        "\n",
        "    model = build_network()\n",
        "\n",
        "   # TODO: update to \"save_path\" you chose in utils script\n",
        "   # we use it to load here your input and labels data,\n",
        "   # pdbs and sequences you created in utils script\n",
        "    data_path = \"/content/drive/MyDrive/protein_hackaton_data/data_to_network\"\n",
        "\n",
        "\n",
        "    # ~~~ load your input and labels: ~~~ # \n",
        "    X = np.load(f\"{data_path}/train_input.npy\") # X = numpy array of shape (1974,NB_MAX_LENGTH,FEATURE_NUM) of all the data input.\n",
        "    Y = np.load(f'{data_path}/train_labels.npy') # Y = numpy array of shape (1974,NB_MAX_LENGTH,OUTPUT_SIZE) of all the data labels.\n",
        "    \n",
        "    # ~~~ split to test and train: ~~~ # \n",
        "    # we split to test and train in a manner that keeps the indices of each set\n",
        "    test_indices = np.random.choice(X.shape[0], X.shape[0] // TEST_PERCENT , replace=False)\n",
        "    X_test = X[test_indices]\n",
        "    y_test = Y[test_indices]\n",
        "    X_train = X[np.logical_not(np.isin(np.arange(X.shape[0]), test_indices))]\n",
        "    y_train = Y[np.logical_not(np.isin(np.arange(X.shape[0]), test_indices))]\n",
        "\n",
        "    X_train = tf.convert_to_tensor(X_train)\n",
        "    X_test = tf.convert_to_tensor(X_test)\n",
        "    y_train = tf.convert_to_tensor(y_train)\n",
        "    y_test = tf.convert_to_tensor(y_test)\n",
        "\n",
        "  #   # ~~~ training: ~~~ #\n",
        "  #   ## if you already have a traind model, move to load model section\n",
        "\n",
        "  #  compile model using Adam optimizer (with learning rate of your choice) and MSE loss.\n",
        "    my_optimizer = tf.keras.optimizers.Adam(learning_rate=LR)\n",
        "    model.compile(optimizer=my_optimizer, loss=\"mse\")\n",
        "\n",
        "    # TODO: give the path to where you want to save your best ckpt:\n",
        "    ckpt_best_path = \"/content/drive/MyDrive/protein_hackaton_data/best_ckpt/\"\n",
        "    model_ckpt = tf.keras.callbacks.ModelCheckpoint(filepath=ckpt_best_path, monitor=\"val_loss\", mode=min, save_best_only=True)\n",
        "\n",
        "    # fit model (use EPOCH for epoch parameter and BATCH for batch_size parameter)\n",
        "    history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH, validation_data=(X_test, y_test), callbacks=[model_ckpt])\n",
        "    \n",
        "    # plot train loss:\n",
        "    plot_val_train_loss(history)\n",
        "    \n",
        "    # ~~~ load model from checkpoint: ~~~ #\n",
        "\n",
        "    # TODO: give the path to exist checkpoint\n",
        "    ckpt_path = \"/content/drive/MyDrive/protein_hackaton_data/best_ckpt\"\n",
        "\n",
        "    # load model\n",
        "    model = tf.keras.models.load_model(ckpt_path)\n",
        "\n",
        "    # fit model (use EPOCH for epoch parameter and BATCH for batch_size parameter)\n",
        "    history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH, validation_data=(X_test, y_test))\n",
        "    \n",
        "    # plot train loss:\n",
        "    plot_val_train_loss(history)\n",
        "\n",
        "    \n",
        "    # ~~~ predictions & evaluating: ~~~ #\n",
        "\n",
        "    # load your sequences and pdb:\n",
        "    # In order to make a prediction and pdb files ot them, you need the sequences\n",
        "    # and pdb names of the sh2 and peptide.\n",
        "    \n",
        "    all_seqs_sh2 = []\n",
        "    with open(f\"{data_path}/all_seqs_sh2.txt\", \"r\") as f:\n",
        "      for line in f:\n",
        "        all_seqs_sh2.append(line.strip())\n",
        "\n",
        "    all_seqs_pep = []\n",
        "    with open(f\"{data_path}/all_seqs_pep.txt\", \"r\") as f:\n",
        "      for line in f:\n",
        "        all_seqs_pep.append(line.strip())\n",
        "\n",
        "    all_pdbs = []\n",
        "    with open(f\"{data_path}/all_pdbs.txt\", \"r\") as f:\n",
        "      for line in f:\n",
        "        all_pdbs.append(line.strip())\n",
        "\n",
        "\n",
        "    # TODO: you can choose here to where you want to save your pdb predictions: \n",
        "    save_path = \"/content/drive/MyDrive/protein_hackaton_data/pred_pdbs/\"\n",
        "\n",
        "    # created pdbs of prediction to test set:    \n",
        "    test_path = f\"{save_path}/test_set\"\n",
        "    isExist = os.path.exists(test_path)\n",
        "    if not isExist:\n",
        "      os.mkdir(test_path)\n",
        "\n",
        "    for i in range(X_test.shape[0]):\n",
        "      X_pred = np.expand_dims(X_test[i], axis=0)\n",
        "      X_pred_sh2 = X_pred[:,:utils_new.SH2_MAX_LEN,:]\n",
        "      X_pred_pep = X_pred[:,utils_new.SH2_MAX_LEN:,:]\n",
        "\n",
        "      Y_pred = model.predict(X_pred, batch_size = (1))\n",
        "      Y_pred_sh2 = Y_pred[:,:utils_new.SH2_MAX_LEN,:]\n",
        "      Y_pred_pep = Y_pred[:,utils_new.SH2_MAX_LEN:,:]\n",
        "\n",
        "      seq_sh2 = all_seqs_sh2[test_indices[i]]\n",
        "      seq_pep = all_seqs_pep[test_indices[i]]\n",
        "      \n",
        "      pdb_name = all_pdbs[test_indices[i]]\n",
        "      path = f\"{test_path}/{pdb_name}\"\n",
        "      isExist = os.path.exists(path)\n",
        "      if not isExist:\n",
        "        os.mkdir(path)\n",
        "\n",
        "      matrix_to_pdb(seq_sh2, Y_pred_sh2[0], f\"{test_path}/{pdb_name}/SH2_pred\")\n",
        "      matrix_to_pdb(seq_pep, Y_pred_pep[0], f\"{test_path}/{pdb_name}/peptide_pred\")\n",
        "\n",
        "    # created pdbs of prediction to train set: \n",
        "    train_path = f\"{save_path}/train_set\"\n",
        "    isExist = os.path.exists(train_path)\n",
        "    if not isExist:\n",
        "      os.mkdir(train_path)\n",
        "\n",
        "    for i in range(X_test.shape[0]):\n",
        "      X_pred = np.expand_dims(X_train[i], axis=0)\n",
        "      X_pred_sh2 = X_pred[:,:utils_new.SH2_MAX_LEN,:]\n",
        "      X_pred_pep = X_pred[:,utils_new.SH2_MAX_LEN:,:]\n",
        "\n",
        "      Y_pred = model.predict(X_pred, batch_size = (1))\n",
        "      Y_pred_sh2 = Y_pred[:,:utils_new.SH2_MAX_LEN,:]\n",
        "      Y_pred_pep = Y_pred[:,utils_new.SH2_MAX_LEN:,:]\n",
        "\n",
        "      train_indices = [i for i in range(X.shape[0]) if i not in test_indices]\n",
        "\n",
        "      seq_sh2 = all_seqs_sh2[train_indices[i]]\n",
        "      seq_pep = all_seqs_pep[train_indices[i]]\n",
        "\n",
        "      pdb_name = all_pdbs[train_indices[i]]\n",
        "      path = f\"{train_path}/{pdb_name}\"\n",
        "      isExist = os.path.exists(path)\n",
        "      if not isExist:\n",
        "        os.mkdir(path)\n",
        "\n",
        "      matrix_to_pdb(seq_sh2, Y_pred_sh2[0], f\"{train_path}/{pdb_name}/SH2_pred\")\n",
        "      matrix_to_pdb(seq_pep, Y_pred_pep[0], f\"{train_path}/{pdb_name}/peptide_pred\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BACKBONE_ATOMS = [\"N\", \"CA\", \"C\", \"O\", \"CB\"]\n",
        "\n",
        "def matrix_to_pdb(seq, coord_matrix, pdb_name):\n",
        "    \"\"\"\n",
        "    Receives a sequence (String) and the output matrix of the neural network (coord_matrix, numpy array)\n",
        "    and creates from them a PDB file named pdb_name.pdb.\n",
        "    :param seq: protein sequence (String), with no padding\n",
        "    :param coord_matrix: output np array of the nanobody neural network, shape = (NB_MAX_LENGTH, OUTPUT_SIZE)\n",
        "    :param pdb_name: name of the output PDB file (String)\n",
        "    \"\"\"\n",
        "    ATOM_LINE = \"ATOM{}{}  {}{}{} {}{}{}{}{:.3f}{}{:.3f}{}{:.3f}  1.00{}{:.2f}           {}\\n\"\n",
        "    END_LINE = \"END\\n\"\n",
        "    k = 1\n",
        "    with open(f\"{pdb_name}.pdb\", \"w\") as pdb_file:\n",
        "        for i, aa in enumerate(seq):\n",
        "            # print(\"aa: \", aa)\n",
        "            third_space = (4 - len(str(i))) * \" \"\n",
        "            for j, atom in enumerate(BACKBONE_ATOMS):\n",
        "                if not (aa == \"G\" and atom == \"CB\"):  # GLY lacks CB atom\n",
        "                    x, y, z = coord_matrix[i][3*j], coord_matrix[i][3*j+1], coord_matrix[i][3*j+2]\n",
        "                    b_factor = 0.00\n",
        "                    first_space = (7 - len(str(k))) * \" \"\n",
        "                    second_space = (4 - len(atom)) * \" \"\n",
        "                    forth_space = (12 - len(\"{:.3f}\".format(x))) * \" \"\n",
        "                    fifth_space = (8 - len(\"{:.3f}\".format(y))) * \" \"\n",
        "                    sixth_space = (8 - len(\"{:.3f}\".format(z))) * \" \"\n",
        "                    seventh_space = (6 - len(\"{:.2f}\".format(b_factor))) * \" \"\n",
        "                    if aa==\"X\":\n",
        "                      three=\"UNK\"\n",
        "                    else:\n",
        "                      three = Polypeptide.one_to_three(aa)\n",
        "                    pdb_file.write(ATOM_LINE.format(first_space, k, atom, second_space, three , \"H\", third_space, \n",
        "                                                    i, forth_space, x, fifth_space, y, sixth_space, z, seventh_space,\n",
        "                                                    b_factor, atom[0]))\n",
        "                    k += 1\n",
        "\n",
        "        pdb_file.write(END_LINE)"
      ],
      "metadata": {
        "id": "ZyrT_q1_CiAq"
      },
      "execution_count": 23,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "net.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}